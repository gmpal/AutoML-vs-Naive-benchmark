{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "import autosklearn\n",
    "import autosklearn.regression\n",
    "from tpot import TPOTRegressor\n",
    "import h2o \n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets us clear the displayed output at every iteration to avoid excessively polluting the notebook \n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML calling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each framework requires a specific format. This is managed with mulitple if conditions in a unique function.\n",
    "def call_automl(framework_name, max_time_given, X_train, y_train, X_test, dataset_name, iteration, identifier):\n",
    "    \n",
    "    if isinstance(y_train,pd.Series):\n",
    "            y_train = y_train.to_frame()\n",
    "    \n",
    "    if framework_name == 'tpot':\n",
    "        x = list(X_train.columns.values)    \n",
    "        y = \"target\"\n",
    " \n",
    "        automl = TPOTRegressor(max_time_mins = max_time_given//60)\n",
    "        automl.fit(X_train,y_train)\n",
    "        \n",
    "        #save leaderboard\n",
    "        automl.export('./exports/'+identifier+'_tpot_exported_pipeline.py')\n",
    "                    \n",
    "        predictions = automl.predict(X_test)    \n",
    " \n",
    "        if len(predictions.shape) > 1:\n",
    "            predictions = np.concatenate(predictions,axis = 0)\n",
    "            \n",
    "    elif framework_name == 'autosklearn':  \n",
    "        automl = autosklearn.regression.AutoSklearnRegressor(\n",
    "            time_left_for_this_task=max_time_given\n",
    "        )\n",
    "        automl.fit(X_train, y_train, dataset_name=dataset_name)\n",
    "        \n",
    "        predictions = automl.predict(X_test)  \n",
    "        \n",
    "        #save leaderboard --> error in saving leaderboard with autosklearn! \n",
    "        #with open(\"./exports/\"+identifier+\".out\", \"w\") as text_file: \n",
    "        #    print(automl.get_models_with_weights())\n",
    "        #    text_file.write(ss)\n",
    "                          \n",
    "    elif framework_name == 'autogluon':  \n",
    "        automl = TabularPredictor(label='target', path='./AutoGluonModel/')\n",
    "        automl.fit(pd.concat([X_train, y_train], axis=1), time_limit=max_time_given)\n",
    "        \n",
    "        #save leaderboard\n",
    "        with open(\"./exports/\"+identifier+\".out\", \"w\") as text_file: #\n",
    "            text_file.write(str(automl.fit_summary(verbosity=1)))\n",
    "        \n",
    "        predictions = automl.predict(X_test)    \n",
    "    \n",
    "    elif framework_name == 'h2o': \n",
    "        \n",
    "        h2o.init() \n",
    "        \n",
    "        x = list(X_train.columns.values)    \n",
    "        y = \"target\"\n",
    " \n",
    "        automl = H2OAutoML(max_runtime_secs = max_time_given)\n",
    "        df = h2o.H2OFrame(pd.concat([X_train, y_train], axis=1))\n",
    "        automl.train(x=x,y=y, training_frame = df)\n",
    " \n",
    "        dft = h2o.H2OFrame(X_test)\n",
    "        predictions = automl.predict(dft)\n",
    "        \n",
    "        lb = automl.leaderboard\n",
    "        \n",
    "        #save leaderboard\n",
    "        with open(\"./exports/\"+identifier+\".out\", \"w\") as text_file: \n",
    "            text_file.write(h2o.as_list(lb.head(rows=lb.nrows)).to_string())\n",
    "     \n",
    " \n",
    "        predictions = h2o.as_list(predictions)\n",
    "        predictions = predictions.values\n",
    "        predictions = np.concatenate( predictions, axis=0 )\n",
    "        \n",
    "    \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp(string):\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    ss = string + \"--> Current Time =\"+ current_time\n",
    "    print(ss)\n",
    "    return ss\n",
    " \n",
    "def train_test_split(considered_slice, fixed_lag, num_test_days, horizon, variables):\n",
    "    considered_slice = get_top_correlated(considered_slice,variables)\n",
    "    _ , sequence = build_XY_multivariate(considered_slice, fixed_lag, horizon)\n",
    "    \n",
    "    dataset = sequence[0] #works both for SISO and MISO (target 0) \n",
    "    \n",
    "    dataset = dataset.rename({'Y0': 'target'}, axis='columns')\n",
    "    # MIMO not implemented\n",
    "    \n",
    "    test_set = dataset.iloc[-num_test_days:,:]\n",
    "    train_set = dataset.iloc[:-num_test_days,:]\n",
    "    y_train = train_set['target'].reset_index(drop=True)\n",
    "    X_train = train_set.drop(['target'], axis=1).reset_index(drop=True)\n",
    "    y_test = test_set['target'].reset_index(drop=True)\n",
    "    X_test = test_set.drop(columns='target').reset_index(drop=True)\n",
    "    \n",
    "        \n",
    "    return  X_train,X_test,y_train,y_test\n",
    "\n",
    "#returns a DF composed by the `num_variables` most correlated columns with column 0, and column 0 itself \n",
    "def get_top_correlated(multi_time_series, num_variables):\n",
    "    s = multi_time_series.corr().abs()[0].sort_values(kind=\"quicksort\",ascending=False)\n",
    "    to_pick = s.index[:num_variables]\n",
    "    return multi_time_series[to_pick]\n",
    "\n",
    "#performs embedding for a single variable given the lag and the horizon\n",
    "def build_XY_univariate(uni_time_series, lag, horizon = 1):\n",
    "    horizon_shift = horizon - 1 \n",
    "    Xy = pd.DataFrame(columns=range(lag))\n",
    "    for i in range(lag):\n",
    "        Xy[i] = uni_time_series[:].shift(-i)\n",
    "    Xy['target'] = uni_time_series[:].shift(- lag - horizon_shift)    \n",
    "    return Xy.dropna()\n",
    " \n",
    "def build_XY_multivariate(multi_time_series,lag , horizon = 1):\n",
    "    \n",
    "    if isinstance(multi_time_series,pd.Series):\n",
    "        multi_time_series = multi_time_series.to_frame()\n",
    " \n",
    "    \n",
    "    num_rows = len(multi_time_series.index) - lag #final matrix rows \n",
    "    num_cols = len(multi_time_series.columns) \n",
    "    \n",
    "    multi_X = pd.DataFrame(index=np.arange(num_rows))\n",
    "    multi_y = pd.DataFrame(index=np.arange(num_rows))\n",
    "    X_names_list = [ \"X\"+str(i)+\"_\"+str(j) for i in range(num_cols) for j in range(lag)]\n",
    "    y_names_list = [ \"Y\"+str(i) for i in range(num_cols)]\n",
    "    \n",
    "    for column_idx in range(num_cols):\n",
    "        single_col = multi_time_series.iloc[:,column_idx]\n",
    "        single_Xy = build_XY_univariate(single_col, lag, horizon)\n",
    "        single_y = single_Xy['target']\n",
    "        single_X = single_Xy.drop(columns ='target')\n",
    "        \n",
    "        multi_X = pd.concat([multi_X,single_X],axis=1)\n",
    "        multi_y = pd.concat([multi_y,single_y],axis=1)\n",
    "    \n",
    "    multi_y.columns = y_names_list\n",
    "    multi_X.columns = X_names_list\n",
    "    \n",
    "    multi_Xy = pd.concat([multi_X,multi_y],axis=1)    \n",
    "    \n",
    "    single_target_list = [pd.concat([multi_X,multi_y[column]],axis=1) for column in multi_y.columns]\n",
    "    return multi_Xy, single_target_list\n",
    "\n",
    "def execute(model_name, max_time_given, series_name, time_series, test_parts, num_test_days, fixed_lag, horizon, variables, identifier):\n",
    "    performances = pd.DataFrame(columns=['Model','Horizon','Variables','Max Time Given','Data','Lag','Test partition','Test Day','Truth','Prediction','Error','Naive pred', 'Naive error', 'MASE'])\n",
    "    window_size = len(time_series) // test_parts\n",
    "    for i in range(test_parts): \n",
    "                \n",
    "        considered_slice = time_series[: window_size * (i+1)]\n",
    "        X_train,X_test,y_train,y_test = train_test_split(considered_slice,fixed_lag,num_test_days,horizon, variables)\n",
    "        \n",
    "        predictions = call_automl(model_name, max_time_given, X_train, y_train, X_test, series_name, i, identifier)\n",
    "        \n",
    "        print(predictions)     \n",
    "        for index_prediction in range(len(predictions)):\n",
    "            \n",
    "            single_pred = predictions[index_prediction]\n",
    "            truth = y_test[index_prediction]\n",
    "            pred_naive = X_test.iloc[index_prediction, (fixed_lag - 1)]\n",
    "            error_model = abs(single_pred - truth )\n",
    "            error_naive = abs(pred_naive - truth)\n",
    "            MASE = error_model/error_naive\n",
    "            \n",
    "            performances.loc[len(performances)] = [ model_name,\n",
    "                                                   horizon,\n",
    "                                                   variables,\n",
    "                                                   max_time_given,\n",
    "                                                   series_name,\n",
    "                                                   fixed_lag,\n",
    "                                                   i,\n",
    "                                                   index_prediction,\n",
    "                                                   truth, \n",
    "                                                   single_pred,\n",
    "                                                   error_model, \n",
    "                                                   pred_naive,\n",
    "                                                   error_naive, \n",
    "                                                   MASE]\n",
    "        \n",
    "    performances.to_csv(\"./results/\"+identifier+\".csv\",index=False)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load unpreprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange = pd.read_csv(\"../../multivariate-time-series-data/exchange_rate/exchange_rate.txt\",header=None)\n",
    "electricity = pd.read_csv(\"../../multivariate-time-series-data/electricity/electricity.txt\",header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fixed_lag = 5\n",
    "test_parts = 3\n",
    "test_size = 17 #total of test_parts * test_size data points in test_parts different moments of time \n",
    "frameworks = [\"autogluon\", \"autosklearn\", \"tpot\", \"h2o\"]\n",
    "datasets_names = [\"Exchange\", \"Electricity\"]\n",
    "datasets = [exchange, electricity]\n",
    "times = [60, 120, 300, 600]\n",
    "variables_num = [1, 3, 5, 8]\n",
    "horizons = [1, 2]\n",
    "\n",
    "#Creates current state log\n",
    "with open(\"current_state.tmp\", \"w\") as text_file: #\n",
    "                        text_file.write( timestamp(\"ABSOLUTE BEGIN\") + \"\\n\")\n",
    "        \n",
    "for max_time_given in times:\n",
    "    for horizon in horizons:\n",
    "        for variables in variables_num:\n",
    "            for framework_name in frameworks:\n",
    "                for index, dataset in enumerate(datasets):\n",
    "                    \n",
    "                    dataset_name = datasets_names[index]\n",
    "                    identifier =  framework_name+'_'+dataset_name+'_h'+str(horizon)+'_v'+str(variables)+'_'+str(max_time_given)+'_l'+str(fixed_lag) \n",
    "                    \n",
    "                    #logs before execution \n",
    "                    with open(\"current_state.tmp\", \"a\") as text_file: #\n",
    "                        text_file.write( timestamp(\"Begin of one cycle\") + ' --- ' + identifier + \"\\n\")\n",
    "                     \n",
    "                    execute(framework_name, max_time_given, dataset_name, dataset, test_parts, test_size, fixed_lag, horizon, variables, identifier)\n",
    "                    \n",
    "                    #logs after execution\n",
    "                    with open(\"current_state.tmp\", \"a\") as text_file: #\n",
    "                        text_file.write( timestamp(\"End of one cycle\") + ' --- ' + identifier + \"\\n\")\n",
    "                    \n",
    "                    clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
